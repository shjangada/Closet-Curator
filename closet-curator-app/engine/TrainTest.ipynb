{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "79dbb874-c939-4b65-a92a-b7483ac3346b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "# Load the embeddings from the .npy file\n",
    "embeddings = np.load(\"embeddings.npy\")\n",
    "\n",
    "# Read the image paths from output.txt\n",
    "with open(\"image_paths.txt\", \"r\") as f:\n",
    "    image_paths = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Ensure the number of embeddings matches the number of image paths\n",
    "if len(embeddings) != len(image_paths):\n",
    "    raise ValueError(\"Number of embeddings does not match the number of image paths.\")\n",
    "\n",
    "# Create a dictionary mapping image names (without .jpg) to embeddings\n",
    "image_to_embedding = {\n",
    "    os.path.splitext(os.path.basename(image_path))[0]: embedding\n",
    "    for image_path, embedding in zip(image_paths, embeddings)\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1098605d-5a9a-4c9d-9bf8-b07031f7454e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example labeled dataset with varying length feature vectors\n",
    "# X = [\n",
    "#     [[0.5, 1.2, 0.7], [1.1, 0.3, 1.4]],  # Two feature vectors\n",
    "#     [[0.2, 0.8, 1.0]],                   # One feature vector\n",
    "#     [[1.4, 0.6, 1.2], [0.9, 1.0, 0.5]],  # Two feature vectors\n",
    "#     [[0.3, 1.1]],                       # One feature vector\n",
    "#     [[0.7, 0.5, 0.9], [0.1, 0.4, 0.6]]   # Two feature vectors\n",
    "# ]\n",
    "\n",
    "# # Labels (0 or 1 indicating compatibility)\n",
    "# y = np.array([1, 0, 1, 0, 1])  # 1 = compatible, 0 = not compatible\n",
    "# Open the file to read the data\n",
    "with open('output.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Initialize the output vectors\n",
    "y = []\n",
    "X = []\n",
    "\n",
    "# Process each line\n",
    "for line in lines:\n",
    "    parts = line.split()  # Split the line into parts\n",
    "    first_item = parts[0]  # The first element, which is always '1'\n",
    "    remaining_items = parts[1:]  # The remaining elements\n",
    "\n",
    "    # Append the first item to the y list and the remaining items to the X list\n",
    "    y.append(first_item)\n",
    "    X.append(remaining_items)\n",
    "\n",
    "# Convert y to integers (since it's all '1', this will create a list of 1's)\n",
    "y = [int(i) for i in y]\n",
    "\n",
    "for row in X:\n",
    "    for i in range(len(row)):\n",
    "        # Replace each element in the row with its corresponding value from the dictionary\n",
    "        row[i] = image_to_embedding[row[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7cfaffa9-f83c-405b-985c-dda60eefcd1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 0.6242, Val Loss: 0.6211, Val Accuracy: 64.56%\n",
      "Epoch 2/50, Train Loss: 0.5786, Val Loss: 0.6104, Val Accuracy: 66.91%\n",
      "Epoch 3/50, Train Loss: 0.5523, Val Loss: 0.6175, Val Accuracy: 64.02%\n",
      "Epoch 4/50, Train Loss: 0.5240, Val Loss: 0.6108, Val Accuracy: 65.19%\n",
      "Epoch 5/50, Train Loss: 0.4970, Val Loss: 0.6196, Val Accuracy: 64.56%\n",
      "Epoch 6/50, Train Loss: 0.4820, Val Loss: 0.6436, Val Accuracy: 65.28%\n",
      "Epoch 7/50, Train Loss: 0.4587, Val Loss: 0.6209, Val Accuracy: 65.83%\n",
      "Epoch 8/50, Train Loss: 0.4315, Val Loss: 0.6386, Val Accuracy: 64.38%\n",
      "Epoch 9/50, Train Loss: 0.3974, Val Loss: 0.6512, Val Accuracy: 64.02%\n",
      "Epoch 10/50, Train Loss: 0.3824, Val Loss: 0.7623, Val Accuracy: 60.78%\n",
      "Epoch 11/50, Train Loss: 0.3694, Val Loss: 0.6935, Val Accuracy: 61.68%\n",
      "Epoch 12/50, Train Loss: 0.3499, Val Loss: 1.0224, Val Accuracy: 57.71%\n",
      "Epoch 13/50, Train Loss: 0.3357, Val Loss: 0.7463, Val Accuracy: 58.97%\n",
      "Epoch 14/50, Train Loss: 0.3247, Val Loss: 0.8408, Val Accuracy: 59.24%\n",
      "Epoch 15/50, Train Loss: 0.2746, Val Loss: 0.8932, Val Accuracy: 60.14%\n",
      "Epoch 16/50, Train Loss: 0.2850, Val Loss: 0.8651, Val Accuracy: 58.43%\n",
      "Epoch 17/50, Train Loss: 0.2306, Val Loss: 1.2104, Val Accuracy: 60.23%\n",
      "Epoch 18/50, Train Loss: 0.2333, Val Loss: 1.0139, Val Accuracy: 59.42%\n",
      "Epoch 19/50, Train Loss: 0.2341, Val Loss: 1.2924, Val Accuracy: 59.69%\n",
      "Epoch 20/50, Train Loss: 0.2026, Val Loss: 1.0908, Val Accuracy: 59.87%\n",
      "Epoch 21/50, Train Loss: 0.1859, Val Loss: 1.1962, Val Accuracy: 58.61%\n",
      "Epoch 22/50, Train Loss: 0.1680, Val Loss: 1.0603, Val Accuracy: 57.98%\n",
      "Epoch 23/50, Train Loss: 0.1888, Val Loss: 0.8043, Val Accuracy: 59.33%\n",
      "Epoch 24/50, Train Loss: 0.1675, Val Loss: 1.1691, Val Accuracy: 59.15%\n",
      "Epoch 25/50, Train Loss: 0.1391, Val Loss: 1.3665, Val Accuracy: 58.61%\n",
      "Epoch 26/50, Train Loss: 0.1345, Val Loss: 1.7406, Val Accuracy: 57.44%\n",
      "Epoch 27/50, Train Loss: 0.1310, Val Loss: 1.9513, Val Accuracy: 57.53%\n",
      "Epoch 28/50, Train Loss: 0.0985, Val Loss: 1.0981, Val Accuracy: 58.34%\n",
      "Epoch 29/50, Train Loss: 0.0933, Val Loss: 1.6820, Val Accuracy: 56.81%\n",
      "Epoch 30/50, Train Loss: 0.0747, Val Loss: 1.8456, Val Accuracy: 58.34%\n",
      "Epoch 31/50, Train Loss: 0.1054, Val Loss: 0.7879, Val Accuracy: 54.73%\n",
      "Epoch 32/50, Train Loss: 0.1180, Val Loss: 1.9049, Val Accuracy: 58.52%\n",
      "Epoch 33/50, Train Loss: 0.0591, Val Loss: 1.5651, Val Accuracy: 57.17%\n",
      "Epoch 34/50, Train Loss: 0.0587, Val Loss: 2.1939, Val Accuracy: 58.34%\n",
      "Epoch 35/50, Train Loss: 0.0667, Val Loss: 2.3135, Val Accuracy: 56.45%\n",
      "Epoch 36/50, Train Loss: 0.1139, Val Loss: 2.1532, Val Accuracy: 56.45%\n",
      "Epoch 37/50, Train Loss: 0.0822, Val Loss: 1.7257, Val Accuracy: 57.44%\n",
      "Epoch 38/50, Train Loss: 0.0483, Val Loss: 2.1926, Val Accuracy: 59.69%\n",
      "Epoch 39/50, Train Loss: 0.0593, Val Loss: 2.1933, Val Accuracy: 57.89%\n",
      "Epoch 40/50, Train Loss: 0.0720, Val Loss: 2.2549, Val Accuracy: 57.98%\n",
      "Epoch 41/50, Train Loss: 0.0715, Val Loss: 2.1724, Val Accuracy: 58.97%\n",
      "Epoch 42/50, Train Loss: 0.0575, Val Loss: 2.1589, Val Accuracy: 58.70%\n",
      "Epoch 43/50, Train Loss: 0.0717, Val Loss: 2.3072, Val Accuracy: 59.15%\n",
      "Epoch 44/50, Train Loss: 0.0522, Val Loss: 2.2109, Val Accuracy: 59.42%\n",
      "Epoch 45/50, Train Loss: 0.0250, Val Loss: 2.5295, Val Accuracy: 56.99%\n",
      "Epoch 46/50, Train Loss: 0.0216, Val Loss: 2.4397, Val Accuracy: 57.62%\n",
      "Epoch 47/50, Train Loss: 0.0412, Val Loss: 2.5156, Val Accuracy: 57.35%\n",
      "Epoch 48/50, Train Loss: 0.0472, Val Loss: 2.4509, Val Accuracy: 58.43%\n",
      "Epoch 49/50, Train Loss: 0.0741, Val Loss: 2.6668, Val Accuracy: 56.54%\n",
      "Epoch 50/50, Train Loss: 0.0837, Val Loss: 2.3263, Val Accuracy: 57.08%\n",
      "Test Loss: 2.1664, Test Accuracy: 60.72%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class DeepSetsLSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, dropout=0.2):\n",
    "        super(DeepSetsLSTMClassifier, self).__init__()\n",
    "        \n",
    "        # Set bidirectional=True to make the LSTM bi-directional\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        # The hidden_dim will now be doubled since it's bi-directional\n",
    "        self.rho = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),  # Multiply hidden_dim by 2\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Dropout(dropout)\n",
    "            \n",
    "        )\n",
    "        \n",
    "        # Change the final layer to have 1 output for binary classification\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass for the DeepSets model with LSTM\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, max_set_size, input_dim)\n",
    "            mask: Boolean mask of shape (batch_size, max_set_size) indicating valid elements\n",
    "        \n",
    "        Returns:\n",
    "            Tensor of shape (batch_size, 1) containing the logit for binary classification\n",
    "        \"\"\"\n",
    "        batch_size, max_set_size, _ = x.shape\n",
    "        \n",
    "        # LSTM processing each element in the set\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # If a mask is provided, apply it to the LSTM output\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(-1)  # Expand mask dimension to match output shape\n",
    "            lstm_out = lstm_out * mask\n",
    "        \n",
    "        # Aggregating the set elements by summing across the set\n",
    "        x_aggregated = torch.sum(lstm_out, dim=1)\n",
    "        \n",
    "        # Pass through the rho network for final features\n",
    "        x = self.rho(x_aggregated)\n",
    "        \n",
    "        # Output a single logit for binary classification\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x  # This is the logit, which will be passed to the loss function\n",
    "\n",
    "class EmbeddingSetDataset(Dataset):\n",
    "    def __init__(self, embedding_sets, labels, max_set_size=None):\n",
    "        \"\"\"\n",
    "        Dataset for handling sets of numpy embedding arrays\n",
    "        \n",
    "        Args:\n",
    "            embedding_sets: List of lists, where each inner list contains numpy arrays (embeddings)\n",
    "            labels: List of labels for each set (0 or 1 for binary classification)\n",
    "            max_set_size: Maximum size for padding sets. If None, uses size of largest set\n",
    "        \"\"\"\n",
    "        self.embedding_sets = embedding_sets\n",
    "        self.labels = labels\n",
    "        self.max_set_size = max_set_size or max(len(s) for s in embedding_sets)\n",
    "        \n",
    "        # Validate that all embeddings have the same dimension\n",
    "        embedding_dims = set(emb.shape[0] for subset in embedding_sets for emb in subset)\n",
    "        if len(embedding_dims) > 1:\n",
    "            raise ValueError(\"All embeddings must have the same dimension\")\n",
    "        self.embedding_dim = embedding_dims.pop()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.embedding_sets)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        embedding_set = self.embedding_sets[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Create padded tensor for the embeddings\n",
    "        padded_set = torch.zeros((self.max_set_size, self.embedding_dim))\n",
    "        mask = torch.zeros(self.max_set_size, dtype=torch.bool)\n",
    "        \n",
    "        # Fill in actual embeddings and mask\n",
    "        for i, embedding in enumerate(embedding_set):\n",
    "            padded_set[i] = torch.from_numpy(embedding).float()\n",
    "            mask[i] = True\n",
    "        \n",
    "        return padded_set, mask, torch.tensor(label)\n",
    "\n",
    "def train_step(model, optimizer, data_loader, device):\n",
    "    \"\"\"Single training epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_data, batch_mask, batch_labels in data_loader:\n",
    "        batch_data = batch_data.to(device)\n",
    "        batch_mask = batch_mask.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(batch_data, batch_mask)\n",
    "        \n",
    "        # Use binary cross entropy loss for binary classification\n",
    "        loss = F.binary_cross_entropy_with_logits(outputs.squeeze(), batch_labels.float())\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "def evaluate(model, data_loader, device):\n",
    "    \"\"\"Evaluation step on the validation or test dataset\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_mask, batch_labels in data_loader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_mask = batch_mask.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "            \n",
    "            outputs = model(batch_data, batch_mask)\n",
    "            loss = F.binary_cross_entropy_with_logits(outputs.squeeze(), batch_labels.float())\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Convert logits to binary predictions (0 or 1)\n",
    "            predicted = torch.round(torch.sigmoid(outputs)).squeeze().long()\n",
    "            total += batch_labels.size(0)\n",
    "            correct += (predicted == batch_labels).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def train_val_test_split(embedding_sets, labels, train_size=0.7, val_size=0.2, test_size=0.1, random_seed=42):\n",
    "    \"\"\"Split data into train, validation, and test sets\"\"\"\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "        embedding_sets, labels, test_size=test_size, random_state=random_seed\n",
    "    )\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_val, y_train_val, test_size=val_size / (train_size + val_size), random_state=random_seed\n",
    "    )\n",
    "    \n",
    "    return (X_train, y_train), (X_val, y_val), (X_test, y_test)\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Example data\n",
    "    (X_train, y_train), (X_val, y_val), (X_test, y_test) = train_val_test_split(X, y)\n",
    "\n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = EmbeddingSetDataset(X_train, y_train)\n",
    "    val_dataset = EmbeddingSetDataset(X_val, y_val)\n",
    "    test_dataset = EmbeddingSetDataset(X_test, y_test)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    # Initialize model with input dimension matching embedding dimension\n",
    "    model = DeepSetsLSTMClassifier(input_dim=1280, hidden_dim=256)  # Adjust hidden_dim as needed\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Train model\n",
    "    num_epochs = 50\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Train step\n",
    "        train_loss = train_step(model, optimizer, train_dataloader, device)\n",
    "        \n",
    "        # Validation step\n",
    "        val_loss, val_accuracy = evaluate(model, val_dataloader, device)\n",
    "        \n",
    "        # Print loss and accuracy\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n",
    "    \n",
    "    # Test model\n",
    "    test_loss, test_accuracy = evaluate(model, test_dataloader, device)\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6003265f-3852-4ea6-8262-52812aeb6440",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
